import tensorflow as tf
import tweepy
import time
import predict
import config

from datetime import datetime, timedelta
from threading import Thread
from queue import Queue

consumer_key = config.CONSUMER_KEY
consumer_secret = config.CONSUMER_SECRET
access_token = config.ACCESS_TOKEN
access_token_secret = config.ACCESS_TOKEN_SECRET

auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)
api = tweepy.API(auth)


"""
yield(): returnã¨é•ã„ã€é–¢æ•°ã‚’ä¸€æ™‚åœæ­¢ã—ã¦ã€æ¬¡ã«å‘¼ã³å‡ºã—ãŸæ™‚ã¯yieldã®æ¬¡ã®è¡Œã‹ã‚‰å§‹ã¾ã‚‹
         å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ã®å—ã‘æ¸¡ã—ã‚’ã€ãƒ¡ãƒ¢ãƒªã‚’æ¶ˆè²»ã›ãšå°‘é‡ãšã¤è¡Œã„ãŸã„ã¨ããªã©ã«æœ‰åŠ¹
"""


# Streamingã‚’ä½¿ã†ç†ç”±ã¯ã€Restã¨é•ã£ã¦æ°¸ç¶šçš„ã«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã‚‹ã‹ã‚‰
# botã¨ã—ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ„ã‚¤ãƒ¼ãƒˆã«ã™ãåå¿œã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã®ã§ã€ã“ã¡ã‚‰ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹
# ãã®ç‚ºã«StreamListenerã‚’ç¶™æ‰¿ã—ãŸã‚¯ãƒ©ã‚¹ã‚’ä½œæˆã™ã‚‹
class MyListener(tweepy.StreamListener):
    # åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰
    def __init__(self, api, queue):
        self.next_tweet_time = self.get_next_tweet_time()
        self.api = api
        self.queue = queue

    # ãƒ„ã‚¤ãƒ¼ãƒˆã‚’å–å¾—ã™ã‚‹ãŸã‚ã®ãƒ¡ã‚½ãƒƒãƒ‰
    def on_status(self, status):
        print("{0}: {1}".format(status.text, status.author.screen_name))

        # è‡ªåˆ†ã®ãƒ„ã‚¤ãƒ¼ãƒˆã¯ç„¡è¦–ã™ã‚‹
        if status.user.id == self.api.me().id:
            print("Ignored my tweet")
            return True
        # ä»–äººãŒè‡ªèº«ã®ãƒ„ã‚¤ãƒ¼ãƒˆã«ãƒªãƒ—ãƒ©ã‚¤ã—ãŸæ™‚ã€queueã«ãƒªãƒ—ãƒ©ã‚¤ã‚’ä¿å­˜
        elif status.text.startswith('@'+self.api.me().screen_name):
            print("Saved mention")
            self.queue.put(status)
            return True

    @staticmethod
    def get_next_tweet_time():
        return datetime.today() + timedelta(hours=4)

    @staticmethod
    def on_error(status_code):
        print("Error: {}".format(status_code))
        return True


# APIã‚’é€šã˜ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ãƒªãƒ—ãƒ©ã‚¤ã‚’å–å¾—ã—ã€queueã«ä¿å­˜ã™ã‚‹ãŸã‚ã®Thread
class StreamReceiverThread(Thread):
    def __init__(self, api, queue):
        super(StreamReceiverThread, self).__init__()
        self.daemon = True
        self.api = api
        self.queue = queue

    def run(self):
        listener = MyListener(self.api, self.queue)
        stream = tweepy.Stream(auth, listener)
        # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã®ãƒ„ã‚¤ãƒ¼ãƒˆã‚’å–å¾—ã—ç¶šã‘ã‚‹
        while True:
            try:
                # userstreamã¯ä½¿ãˆãªããªã£ãŸãŸã‚ã€filterã«ã‚ˆã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ãŒå¿…é ˆ
                # ãã®ãŸã‚ã€è‡ªåˆ†ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼åã§æ¤œç´¢ã‚’ã‹ã‘ã¦ã„ã‚‹
                stream.filter(track=['@'+self.api.me().screen_name])
            except Exception as e:
                print(e)
                print(e.__doc__)
                time.sleep(60)
                stream = tweepy.Stream(auth, listener)


# queueã«ä¿å­˜ã•ã‚ŒãŸãƒªãƒ—ãƒ©ã‚¤ã«è¿”ä¿¡ã™ã‚‹ãŸã‚ã®Thread
class ProcessingThread(Thread):
    def __init__(self, queue):
        super(ProcessingThread, self).__init__()
        self.daemon = True
        self.queue = queue

    # queueã«ä¿å­˜ã•ã‚ŒãŸãƒªãƒ—ãƒ©ã‚¤ã‚’éåŒæœŸã«å–å¾—ã—ã€seq2seqã§è¿”ä¿¡ã‚’ç”Ÿæˆ
    def run(self):
        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.2)
        tf_config = tf.ConfigProto(gpu_options=gpu_options)
        with tf.Session(config=tf_config) as sess:
            # predictor = å­¦ç¿’æ¸ˆã¿ã®seq2seq
            predictor = predict.EasyPredictor(sess)
            while True:
                reply = self.queue.get()
                make_reply_text(reply, predictor)


# ãƒªãƒ—ãƒ©ã‚¤ã‚’æŠ•ç¨¿ã™ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰
def post_reply(reply_body, screen_name, status_id):
    # ãƒªãƒ—ãƒ©ã‚¤ã®æ–‡ç« 
    reply_body = reply_body.replace('_UNK', 'ğŸ˜„')
    reply_text = "@" + screen_name + " " + reply_body
    print("Reply:{0}".format(reply_text))
    # ãƒªãƒ—ãƒ©ã‚¤ã‚’æŠ•ç¨¿
    api.update_status(status=reply_text, in_reply_to_status_id=status_id)


# GPUã‚’ä½¿ç”¨ã—ã¦ã€seq2seqã®ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰è¿”ç­”ã‚’äºˆæ¸¬ã—ã€ãƒªãƒ—ãƒ©ã‚¤ã¨ã—ã¦è¿”ã™ãƒ¡ã‚½ãƒƒãƒ‰
def make_reply_text(tweet, predictor):

    print("Processing {0}...".format(tweet.text))
    screen_name = tweet.author.screen_name
    # è¿”ç­”ã‚’äºˆæ¸¬
    replies = predictor.predict(tweet.text)
    if not replies:
        print("no reply")
    reply_body = replies[0]
    if reply_body is None:
        print("No reply predicted")
    else:
        try:
            # twitterã«è¿”ç­”ã‚’æŠ•ç¨¿
            post_reply(reply_body, screen_name, tweet.id)
        except tweepy.TweepError as e:
            print(e)
            print(e.__doc__)


# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã«ãƒªãƒ—ãƒ©ã‚¤ã«è¿”ä¿¡ã™ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰
def start_streaming():
    # ãƒ„ã‚¤ãƒ¼ãƒˆã‚’ä¿å­˜ã™ã‚‹ãŸã‚ã®queueã‚’ç”Ÿæˆ
    q = Queue()
    # ãƒ„ã‚¤ãƒ¼ãƒˆå–å¾—ç”¨Threadã¨ãƒªãƒ—ãƒ©ã‚¤ç”¨Threadã‚’ç”Ÿæˆ
    p_thread = ProcessingThread(q)
    s_thread = StreamReceiverThread(api, q)
    p_thread.start()
    s_thread.start()
    print("start streaming")
    while True:
        time.sleep(1)


if __name__ == '__main__':
    start_streaming()
